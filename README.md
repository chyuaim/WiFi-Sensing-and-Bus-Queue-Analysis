# WiFi Sensing and Bus Queue Analysis

## 1. Data Processing
The data processing part contains 5 python scripts. The data_pool.py is used to receive incoming real-time data. The gen_filter_list.py script is used to generate a list of MAC addresses for other scripts to identify unwanted signals. The process_south.py script is used to process data from the south gate while the process_north.py script is used to process data from the north gate. All data_pool.py, gen_filter_list.py, process_south.py and process_north.py require config.py to get the functions and global variables.

### 1.1 data_pool.py

The script is designed to receive data in real-time. It keeps looping and waiting for new connections from MTrec to send new data of the WiFi sensors. The data is received one by one in json format and is stored to a list, every minute, the stored data is pre-processed and saved to a folder in the database corresponding to its date and the file name is the timestamp of the first data in the list.

### 1.2 gen_filter_list.py

Every time the script runs, it reads the data of past 72 hours from the database and retrieves the target MAC addresses we want to filter. The target MAC addresses include: 
1. devices exist during midnight
2. devices exist in at least 6 different hour slot
3. devices stay longer than 30 minutes during the past 90 minutes.

The MAC addresses of the devices which satisfy any of the above criteria is stored in the filter_list.txt file located in the /resources folder.
database
### 1.3 process_south.py

This is the main script to process data from the South Gate. The script loops every minute and reads the data of the past 30 minutes from the database, the data is then filtered according to the filter_list.txt as well as some other filter criteria. Upon filtered, the data is classified to 4 different zones, corresponds to the 4 bus stops in South Gate. The classified data are upsampled and the results (no. of passengers in each second and the queue time distribution of the past minute) are derived from the dataset. The results are then uploaded to the cloud database.

### 1.4 process_north.py

This is the main script to process data from the North Gate. Most parts of the scirpt are identical to process_south.py except the data in question is changed to North Gate and zone classification is not applied to the North Gate data.

### 1.5 config.py

This file contains multiple functions and global variables which are used by the other 4 scripts. It has to be placed along with the other 4 scripts so that the scripts can run properly.

## 2. Resources and Upstart

### 2.1 resources

The resources folder contains the following files:
1. filter_list.txt 
    - generated by gen_filter_list.py
    - used by process_south.py and process_north.py
2. mac_prefix.txt
    - contains all MAC address prefix which are possibly from a mobile device
    - generated according to https://macaddress.io/database-download
    - used by process_south.py and process_north.py
3. model
    - the weightings of the classification model
    - generated from the zone_classificaition.ipynb in the supplement folder
    - used by process_south.py

### 2.2 upstart

The upstart contains all the .conf files so that the .py files can keep running in the system.

## 3. Restapi
The restful api accepts queries for the latest records of the queuing numbers and queue time distribution, also the last hour records. 

## 4. Bokeh

The bokeh folder contains all the scripts for plotting the numbers and distributions. Each .py script is responsible for one chart. The folder also includes the bokeh.service to keep the scripts running and host a website at a single port for all of the charts.

## 5. Deployment

### 5.1 Server setup

1. Programming packages
    - Operating System: Ubuntu 14.04

    - Programming Language: Python 3.5

    - Python Libarires: pandas, numpy, torch, scikit-learn, pymongo
    ```
    sudo apt update
    ```

    ```
    sudo apt install python3-pip
    ```
    ```
    pip3 install pandas numpy torch scikit-learn pymongo bokeh
    ```
    ```
    sudo apt install nodejs
    sudo apt install npm
    ```

2. Folder location
    
    The scripts are put in ~/FYP in the system, cd to the directory to continue
    ```
    cd ~/FYP
    ```

### 5.2 Start the scripts

We use the .conf files to keep the scripts running.

1. Copy the .conf files to /etc/init
    ```
    sudo cp upstart/data_pool.conf /etc/init
    sudo cp upstart/gen_filter_list.conf /etc/init
    sudo cp upstart/process_south.conf /etc/init
    sudo cp upstart/process_north.conf /etc/init
    sudo cp bokeh/bokeh.conf /etc/init
    sudo cp restapi/restapi.conf /etc/init
    ```

2. Start the service
    ```
    sudo service data_pool start
    sudo service gen_filter_list start
    sudo service process_south start
    sudo service process_north start
    sudo service restapi start
    sudo service bokeh start
    ```